{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrsJrPW4jBbuAYJVQtK0mE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaygopan/Akku/blob/main/Akku_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YzOEGEbpGES"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sentence-transformers\n",
        "!pip install \"tensorflow-text==2.11.*\"\n",
        "!pip install torch==2.1.0\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as t\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "#import senteval\n",
        "import time\n",
        "import requests\n",
        "import numpy as np\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "metadata": {
        "id": "ySxpJfxlpmvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask"
      ],
      "metadata": {
        "id": "EWyi5XZIpnAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_list = ['sentence-transformers/bert-base-nli-mean-tokens'\n",
        "              #'sentence-transformers/all-MiniLM-L6-v2',\n",
        "              #'sentence-transformers/all-mpnet-base-v2',\n",
        "              #'jinaai/jina-embeddings-v2-base-en']\n",
        "              #'hkunlp/instructor-xl',\n",
        "              #'sentence-transformers/paraphrase-multilingual-mpnet-base-v2']\n",
        "              #'SupstarZh/whitenedcse-bert-base',\n",
        "              #'SupstarZh/whitenedcse-bert-large',\n",
        "              #'bert-base-uncased'\n",
        "              ]\n",
        "count = 1\n",
        "models = {}\n",
        "for model in model_list:\n",
        "  models[model] = {'Model': AutoModel.from_pretrained(model) , 'Tokenizer' : AutoTokenizer.from_pretrained(model)}\n",
        "  count = count +1"
      ],
      "metadata": {
        "id": "NuEIsCJbpnC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models.keys()"
      ],
      "metadata": {
        "id": "P-Q6VvF4pnFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk"
      ],
      "metadata": {
        "id": "djqHGtirpnIh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "-qkvSNI_pnKk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub('<.*?>', '', text)\n",
        "    # Remove punctuation and other non-alphabetic characters\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "irqpj8wSpnMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models['sentence-transformers/bert-base-nli-mean-tokens']['Model']\n",
        "tokenizer = models['sentence-transformers/bert-base-nli-mean-tokens']['Tokenizer']"
      ],
      "metadata": {
        "id": "GMpS0JhUqNMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoding(text):\n",
        "    #Tokenize sentences\n",
        "    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt',is_split_into_words=True)\n",
        "\n",
        "    #Compute token embeddings\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "\n",
        "    #Perform pooling. In this case, mean pooling\n",
        "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "    return sentence_embeddings"
      ],
      "metadata": {
        "id": "tqB5TVXXqNPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_row(text):\n",
        "  preprocessed = preprocess_text(text)\n",
        "  embedding = get_encoding(preprocessed)\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "MiO6WjsCqNRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "id": "UPg8YpCVqNUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text file line by line\n",
        "with open('data.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Convert each line to a DataFrame\n",
        "data = []\n",
        "for line in lines:\n",
        "    # Assuming each line contains comma-separated values\n",
        "    values = line.strip().split(',')  # adjust delimiter if needed\n",
        "    data.append(values)\n",
        "\n",
        "# Convert the list of lists to a pandas DataFrame\n",
        "df = pd.DataFrame(data, columns = ['Text'])\n",
        "\n",
        "# Optionally, you can specify column names if needed\n",
        "# For example, if your first line contains column headers:\n",
        "# df.columns = ['Column1', 'Column2', ...]\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "id": "lJntJ_1KqNWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Embedding'] = df['Text'].apply(embed_row)"
      ],
      "metadata": {
        "id": "YKoO8kuZqb-v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}